{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Coding Challenge\n",
        "\n",
        "## Objective:\n",
        "Prepare a dataset for training an ML model on the airfRANS data.\n",
        "\n",
        "The airfRANS dataset is a collection of airfoils and their RANS simulation solution. The [documentation](https://airfrans.readthedocs.io/en/latest/notes/dataset.html) describes the dataset and the functionality in the Python library they've built in detail.\n",
        "\n",
        "Ask:\n",
        "* Create a dataset for training an ML model using the airfRANS dataset.\n",
        "  * The dataset should provide a sequence of points with their SDF (distance from the airfoil) value as the input `(x, y, sdf)` and the velocity `(x, y, v_x, v_y`) as the target. Package the data such that it can be quickly loaded for training a model.\n",
        "* Provide some dataset statistics to help users understand the data.\n",
        "* Document your design decisions.\n",
        "\n",
        "### Note:\n",
        "This is an intentionally open-ended challenge. The primary objective is to see how you write code. Think of this as an evaluation of your ability to write code for a production environment.\n",
        "\n",
        "## Time\n",
        "This challenge is designed to take ~2-3 hours. If it's taking much longer than that, feel free to stop and document what your next steps would have been.\n",
        "\n",
        "## Deliverables\n",
        "Your choice. You can send us a github repo, jupyter notebook, or just raw files. Do whatever you think will best demonstrate your SW engineering skills."
      ],
      "metadata": {
        "id": "tVaH6qoo_vtm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing\n",
        "The below provides some sample code to help with basic dataset loading of the airfrans dataset."
      ],
      "metadata": {
        "id": "ulA8ezF_-qEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install airfrans --quiet\n",
        "!pip install pyvista --quiet\n",
        "!sudo apt install libgl1-mesa-glx xvfb"
      ],
      "metadata": {
        "id": "5LjkHnmO3afC",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90715b4b-aa91-4936-91dd-218a8dfe7d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings\n",
            "  xfonts-utils xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libgl1-mesa-glx libxfont2 libxkbfile1 x11-xkb-utils xfonts-base\n",
            "  xfonts-encodings xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 10 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 7,820 kB of archives.\n",
            "After this operation, 12.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgl1-mesa-glx amd64 23.0.4-0ubuntu1~22.04.1 [5,584 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.12 [28.7 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.12 [864 kB]\n",
            "Fetched 7,820 kB in 1s (6,316 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 10.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 123623 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
            "Preparing to unpack .../1-libgl1-mesa-glx_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../2-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../3-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../4-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../5-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../6-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../7-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../8-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.12_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../9-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.12_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6h-7WQ903Ahu"
      },
      "outputs": [],
      "source": [
        "import airfrans as af\n",
        "import os\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This has already been run\n",
        "# Download the dataset\n",
        "# NOTE: Dataset documentation can be found here: https://airfrans.readthedocs.io/en/latest/notes/dataset.html\n",
        "\n",
        "directory_name = Path('airfrans/')\n",
        "file_name = 'Dataset'\n",
        "if not directory_name.exists() or not any(directory_name.iterdir()):\n",
        "  af.dataset.download(root=str(directory_name), file_name=file_name, unzip=True, OpenFOAM=False)"
      ],
      "metadata": {
        "id": "Sigc43-13WRW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "244a2b8f-112f-4860-8652-3398ec8b0168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading AirfRANS: 9.34GB [07:17, 22.9MB/s]                            \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Dataset.zip at airfrans...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: we use the `reynolds` task here to prevent a memory error with Colab. Feel free to use `full` if doing this offline.\n",
        "dataset_list, dataset_name = af.dataset.load(root=str(directory_name/file_name), task = 'reynolds', train = True)"
      ],
      "metadata": {
        "id": "liv0xrzF9G7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08e691e1-0946-45a4-bf76-78938a147895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading dataset (task: reynolds, split: train): 100%|██████████| 504/504 [04:30<00:00,  1.86it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some set basics!\n",
        "\n",
        "T = 298.15 # As recommended in original documentation\n",
        "inlet_velocity = []\n",
        "angle_attack = []\n",
        "digits4 = []\n",
        "digits5 = []\n",
        "\n",
        "for item in dataset_name:\n",
        "    params = item.split('_')\n",
        "    inlet_velocity.append(float(params[2]))\n",
        "    angle_attack.append(float(params[3]))\n",
        "\n",
        "    if len(params) == 7:\n",
        "        digits4.append(list(map(float, params[-3:])))\n",
        "    else:\n",
        "        digits5.append(list(map(float, params[-4:])))\n",
        "\n",
        "print(\"\\n--- ♥ Dataset Overview ♥ ---\")\n",
        "print(f\"\\nDataset_name: {type(dataset_name)} containing elements of type {type(dataset_name[0])}\")\n",
        "print(f\"\\n\\tExample element: {dataset_name[0]}\")\n",
        "print(f\"\\tLength: {len(dataset_name):,} elements\")\n",
        "print(f\"\\t- NACA 4-digit count: {len(digits4):,}\")\n",
        "print(f\"\\t- NACA 5-digit count: {len(digits5):,}\")\n",
        "print(f\"\\tInlet velocity range: {min(inlet_velocity):.2f} to {max(inlet_velocity):.2f}\")\n",
        "print(f\"\\tAngle of attack range: {min(angle_attack):.2f} to {max(angle_attack):.2f}\")\n",
        "\n",
        "print(f\"\\nDataset_list: {type(dataset_list)} containing elements of type {type(dataset_list[0])}\")\n",
        "print(f\"\\n\\tLength: {len(dataset_list):,} elements\")\n",
        "print(f\"\\tShape of each element: (N, 12)\")\n",
        "print(f\"\\t-  N: number of points in simulation\")\n",
        "print(f\"\\t- 12: number of features\")\n",
        "print(f\"\\tNumber of points in first simulation as reference: {len(dataset_list[0][:]):,}\")\n",
        "print(f\"\\t- Note: number of points in simulations vary a bit\")\n",
        "\n",
        "features = [\n",
        "    [\"\\tPosition x\", \"Position y\", \"Inlet velocity\", \"Inlet velocity\"],\n",
        "    [\"\\tDist to airfoil\", \"Normals a\", \"Normals b\", \"Velocity x\"],\n",
        "    [\"\\tVelocity y\", \"Pressure/mass\", \"Kin. viscosity\", \"Bool*\"]\n",
        "]\n",
        "\n",
        "print(\"\\n\\tDescription of 12 features, in order: \")\n",
        "print('\\t\\n'.join(['\\t'.join([str(cell) for cell in row]) for row in features]))\n",
        "print(\"\\t\\n\\t*Evals to true if point lies on the airfoil\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtSCCrt7uVoB",
        "outputId": "ebfb8764-35fd-4628-c49c-c0f77e8f7ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ♥ Dataset Overview ♥ ---\n",
            "\n",
            "Dataset_name: <class 'list'> containing elements of type <class 'str'>\n",
            "\n",
            "\tExample element: airFoil2D_SST_58.831_-3.563_2.815_4.916_10.078\n",
            "\tLength: 504 elements\n",
            "\t- NACA 4-digit count: 239\n",
            "\t- NACA 5-digit count: 265\n",
            "\tInlet velocity range: 46.84 to 77.95\n",
            "\tAngle of attack range: -4.93 to 14.79\n",
            "\n",
            "Dataset_list: <class 'list'> containing elements of type <class 'numpy.ndarray'>\n",
            "\n",
            "\tLength: 504 elements\n",
            "\tShape of each element: (N, 12)\n",
            "\t-  N: number of points in simulation\n",
            "\t- 12: number of features\n",
            "\tNumber of points in first simulation as reference: 170,180\n",
            "\t- Note: number of points in simulations vary a bit\n",
            "\n",
            "\tDescription of 12 features, in order: \n",
            "\tPosition x\tPosition y\tInlet velocity\tInlet velocity\t\n",
            "\tDist to airfoil\tNormals a\tNormals b\tVelocity x\t\n",
            "\tVelocity y\tPressure/mass\tKin. viscosity\tBool*\n",
            "\t\n",
            "\t*Evals to true if point lies on the airfoil\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input:   dataset_list\n",
        "# Output:  tensor(input: x, y, sdf), tensor(target: x, y, v_x, v_y)\n",
        "#\n",
        "# Challenges:\n",
        "#   1. Memory Usage\n",
        "#      - `af.dataset.load` pulls all data into RAM, little left to play with.\n",
        "#   2. Data Structure\n",
        "#      - Indexing is a bit complex because our dataset is a list of ndarray simulations.\n",
        "#        For example, to find the 500,000th data point, we need to find the index of the simulation it belongs to first.\n",
        "#        This is challenging, because each simulation has a different number of data points.\n",
        "#\n",
        "# Approach:\n",
        "#   First idea: Create a dataset to hold target features (position, sdf, v_pos) and simply run the ML model on that.\n",
        "#      - Pros: Straightforward, readable code; still a viable option.\n",
        "#      - Cons: Requires copying data, which quickly exceeded Google Colab memory limits. (No $$.)\n",
        "#      - Next steps: Consider chunking the data, possibly loading only parts at a time to avoid RAM overload.\n",
        "#\n",
        "# Solution:\n",
        "#   Avoid making any copies and directly reference data in RAM.\n",
        "#      - Pros: Keeps RAM usage low! No need to spend money.\n",
        "#      - Cons: Less readable code. (I sorry <3)\n",
        "#      - Details: Solved the indexing problem without data copies by mapping a given index to a `sim_id` and `row`.\n",
        "#                Stored valid index ranges of each simulation (e.g., sim 0: [0, 170180], sim 1: [170181, 320912], ...) in an array,\n",
        "#                and used binary search to find `sim_id`.\n",
        "#      - Next steps: Do sanity check with teammates who understand physics part of it. Test the current code and explore how\n",
        "#        Geometric Deep Learning libraries can help!\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class AirfoilDataset(Dataset):\n",
        "    def __init__(self, dataset_list):\n",
        "        self.dataset_list = dataset_list\n",
        "        self.sim_bounds = self._find_sim_bounds()\n",
        "\n",
        "    def _find_sim_bounds(self):\n",
        "        bound, s_b = 0, []\n",
        "        for sim in self.dataset_list:\n",
        "            # Find length of current simulation and add to total\n",
        "            bound += len(sim)\n",
        "\n",
        "            # Record boundary index of current simulation\n",
        "            s_b.append(bound)\n",
        "        return s_b\n",
        "\n",
        "    def _bin_search(self, left, right, arr, target):\n",
        "        mid = left + (right - left) // 2\n",
        "\n",
        "        if arr[mid] <= target and target < arr[mid+1]:\n",
        "            # Target found\n",
        "            return mid+1\n",
        "\n",
        "        elif  arr[mid] < target:\n",
        "            # Target bigger than midpoint\n",
        "            return self._bin_search(mid + 1, right, arr, target)\n",
        "\n",
        "        else:\n",
        "            # Target smaller than midpoint\n",
        "            return self._bin_search(left, mid - 1, arr, target)\n",
        "\n",
        "        # Target not found anywhere\n",
        "        return -1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.sim_bounds[-1]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx < self.sim_bounds[0]:\n",
        "            # We can skip search\n",
        "            sim_id = 0\n",
        "            row = idx\n",
        "        else:\n",
        "            # Find index of target simulation\n",
        "            sim_id = self._bin_search(0, len(self.dataset_list)-1, self.sim_bounds, idx)\n",
        "\n",
        "            # Find index of target datapoint within simulation\n",
        "            row = idx - self.sim_bounds[sim_id-1]\n",
        "\n",
        "        x = self.dataset_list[sim_id][row][0] # Used this approach instead of af.Simulation to maintain encapsulation\n",
        "        y = self.dataset_list[sim_id][row][1]\n",
        "        sdf = self.dataset_list[sim_id][row][4]\n",
        "        v_x = self.dataset_list[sim_id][row][7]\n",
        "        v_y = self.dataset_list[sim_id][row][8]\n",
        "\n",
        "        # Put everything in tensor\n",
        "        input = torch.cat([torch.tensor((x, y, sdf), dtype = torch.float)])\n",
        "        target = torch.cat([torch.tensor((x, y, v_x, v_y), dtype = torch.float)])\n",
        "        return input, target\n"
      ],
      "metadata": {
        "id": "WzgzatWnbJju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example output\n",
        "\n",
        "dataset = AirfoilDataset(dataset_list)\n",
        "print(dataset.__getitem__(0))"
      ],
      "metadata": {
        "id": "lT_uLpAElfUV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d17d160e-0272-4eac-e4b3-a7206ebe2b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([ 4.2169, -0.1999,  3.2231]), tensor([ 4.2169, -0.1999, 54.5453, -3.3534]))\n"
          ]
        }
      ]
    }
  ]
}